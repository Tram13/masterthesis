{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import optim, nn\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm  # TODO: niet meer notebook als dat niet nodig is\n",
    "\n",
    "from src.predictor.multilayer_perceptron import MultiLayerPerceptronPredictor\n",
    "from src.data.data_preparer import DataPreparer\n",
    "from src.data.data_reader import DataReader\n",
    "from src.tools.RestaurantReviewsDataset import RestaurantReviewsDataset\n",
    "\n",
    "while str(os.getcwd())[-3:] != 'src':  # Execute from src-directory root\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "businesses, reviews, tips = DataReader().read_data()\n",
    "input_ml_train, input_ml_test, output_ml_train, output_ml_test = DataPreparer.get_train_test_validate(businesses, reviews, tips)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def train_epoch(model: nn.Module, dataloader: DataLoader, loss_fn, optimizer: Optimizer) -> float:\n",
    "    model.train()  # Prepare layers of model for training\n",
    "    # Prepare statistics\n",
    "    total_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    for restaurant_reviews, ratings in tqdm(dataloader, desc=f\"Training network in batches\", leave=None):\n",
    "        # Prepare data\n",
    "        restaurant_reviews, ratings = DataPreparer.get_tensor_for_ml(restaurant_reviews, ratings)\n",
    "        # Compute predictions and loss\n",
    "        predictions = model(restaurant_reviews)\n",
    "        loss = loss_fn(predictions, ratings)\n",
    "        # Compute statistics\n",
    "        total_loss += loss.item()\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    mean_loss = total_loss / num_batches\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "def validate_epoch(model: nn.Module, dataloader: DataLoader, loss_fn) -> tuple[float, float]:\n",
    "    model.eval()  # Prepare layers of model for evaluation\n",
    "    # Prepare statistics\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for restaurant_reviews, ratings in dataloader:\n",
    "            # Prepare data\n",
    "            restaurant_reviews, ratings = DataPreparer.get_tensor_for_ml(restaurant_reviews, ratings)\n",
    "            # Compute predictions and loss\n",
    "            predictions = model(restaurant_reviews)\n",
    "            loss = loss_fn(predictions, ratings)\n",
    "            # Calculate statistics\n",
    "            total_loss += loss.item()\n",
    "            correct += ((ratings - 0.125 <= predictions) & (predictions <= ratings + 0.125)).type(torch.float).sum().item()\n",
    "\n",
    "    mean_loss = total_loss / num_batches\n",
    "    accuracy = correct / size\n",
    "    return mean_loss, accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# splits = KFold(n_splits=10, shuffle=True)\n",
    "# history = {'train_loss': [], 'test_loss': [],'test_acc':[]}\n",
    "# batch_size = 1024\n",
    "# epochs = 10\n",
    "# criterion = nn.MSELoss()\n",
    "#\n",
    "# for i, (train_idx_fold, val_idx_fold) in tqdm(enumerate(splits.split(input_ml_train, output_ml_train)), desc=\"K-Fold Cross Validation\"):\n",
    "#\n",
    "#     train_data_fold = RestaurantReviewsDataset(input_ml_train.iloc[train_idx_fold].to_numpy(), output_ml_train.iloc[train_idx_fold].to_numpy())\n",
    "#     validate_data_fold = RestaurantReviewsDataset(input_ml_train.iloc[val_idx_fold].to_numpy(), output_ml_train.iloc[val_idx_fold].to_numpy())\n",
    "#\n",
    "#     train_loader = DataLoader(train_data_fold, batch_size=batch_size)\n",
    "#     val_loader = DataLoader(validate_data_fold, batch_size=batch_size)\n",
    "#\n",
    "#     model = MultiLayerPerceptronPredictor(input_size=input_ml_train.columns.size, output_size=1)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "#\n",
    "#     for epoch in tqdm(range(epochs), desc=\"Epochs\"):  # TODO: uitzoeken hoe het zit met epochs en kfolds, en in welke volgorde ik die run, ALS die gecombineerd mogen zelfs, prob niet lijkt me nu\n",
    "#         train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "#         test_loss, test_acc = validate_epoch(model, val_loader, criterion)\n",
    "#\n",
    "#         print(f\"Epoch:{epoch + 1}/{epochs} AVG Training Loss:{train_loss:.3f} AVG Test Loss:{test_loss:.3f} AVG Test Acc {test_acc * 100:.2f} %\")\n",
    "#         history['train_loss'].append(train_loss)\n",
    "#         history['test_loss'].append(test_loss)\n",
    "#         history['test_acc'].append(test_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1bcbae589fc4d1b802bff3ffbc1754f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training network in batches:   0%|          | 0/3697 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6fa294422e3943df98dee05c4def6d2e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training network in batches:   0%|          | 0/3697 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b9762175a8c4cafb52607cf2e5f2a2c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training network in batches:   0%|          | 0/3697 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8676bb276903406ea6db4cee63f525df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 17\u001B[0m\n\u001B[0;32m     15\u001B[0m epochs_with_progressbar \u001B[38;5;241m=\u001B[39m tqdm(\u001B[38;5;28mrange\u001B[39m(epochs), desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpochs\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m epochs_with_progressbar:\n\u001B[1;32m---> 17\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m     test_loss, test_acc \u001B[38;5;241m=\u001B[39m validate_epoch(model, val_loader, criterion)\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;66;03m# print(f\"Epoch:{epoch + 1}/{epochs} AVG Training Loss:{train_loss:.3f} AVG Test Loss:{test_loss:.3f} AVG Test Acc {test_acc * 100:.2f} %\")\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[4], line 6\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(model, dataloader, loss_fn, optimizer)\u001B[0m\n\u001B[0;32m      4\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m      5\u001B[0m num_batches \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(dataloader)\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m restaurant_reviews, ratings \u001B[38;5;129;01min\u001B[39;00m tqdm(dataloader, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining network in batches\u001B[39m\u001B[38;5;124m\"\u001B[39m, leave\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;66;03m# Prepare data\u001B[39;00m\n\u001B[0;32m      8\u001B[0m     restaurant_reviews, ratings \u001B[38;5;241m=\u001B[39m DataPreparer\u001B[38;5;241m.\u001B[39mget_tensor_for_ml(restaurant_reviews, ratings)\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;66;03m# Compute predictions and loss\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Documents\\Projects\\MasterThesis\\.venv_windows\\lib\\site-packages\\tqdm\\notebook.py:259\u001B[0m, in \u001B[0;36mtqdm_notebook.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    258\u001B[0m     it \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m(tqdm_notebook, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__iter__\u001B[39m()\n\u001B[1;32m--> 259\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m it:\n\u001B[0;32m    260\u001B[0m         \u001B[38;5;66;03m# return super(tqdm...) will not catch exception\u001B[39;00m\n\u001B[0;32m    261\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m    262\u001B[0m \u001B[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Documents\\Projects\\MasterThesis\\.venv_windows\\lib\\site-packages\\tqdm\\std.py:1195\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1192\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[0;32m   1194\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1195\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[0;32m   1196\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m   1197\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[0;32m   1198\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Documents\\Projects\\MasterThesis\\.venv_windows\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mD:\\Documents\\Projects\\MasterThesis\\.venv_windows\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mD:\\Documents\\Projects\\MasterThesis\\.venv_windows\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mD:\\Documents\\Projects\\MasterThesis\\.venv_windows\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mD:\\Documents\\Projects\\MasterThesis\\src\\tools\\RestaurantReviewsDataset.py:14\u001B[0m, in \u001B[0;36mRestaurantReviewsDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[1;32m---> 14\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx_train[idx], \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43my_train\u001B[49m[idx]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "history = {'train_loss': [], 'test_loss': [],'test_acc':[]}\n",
    "batch_size = 1024\n",
    "epochs = 50\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_data = RestaurantReviewsDataset(input_ml_train.to_numpy(), output_ml_train.to_numpy())\n",
    "validate_data = RestaurantReviewsDataset(input_ml_train.to_numpy(), output_ml_train.to_numpy())\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "val_loader = DataLoader(validate_data, batch_size=batch_size)\n",
    "\n",
    "model = MultiLayerPerceptronPredictor(input_size=input_ml_train.columns.size, output_size=1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "epochs_with_progressbar = tqdm(range(epochs), desc=\"Epochs\")\n",
    "for epoch in epochs_with_progressbar:\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    test_loss, test_acc = validate_epoch(model, val_loader, criterion)\n",
    "\n",
    "    # print(f\"Epoch:{epoch + 1}/{epochs} AVG Training Loss:{train_loss:.3f} AVG Test Loss:{test_loss:.3f} AVG Test Acc {test_acc * 100:.2f} %\")\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    epochs_with_progressbar.set_description_str(f\"Epochs (loss of last 5 epochs: {history['test_loss'][-5:]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
