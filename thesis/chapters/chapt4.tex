\chapter{Experimenten}
% TODO ABBREVIATIONS
% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS% TODO ABBREVIATIONS
\section{Voorgestelde architectuur}
In deze thesis onderzoeken we of de combinatie van tekstuele data aan de hand van transformermodellen kan omgezet worden in features, die het voorspellingsvermogen van een neuraal netwerk positief beïnvloeden. Ons basisidee ziet er uit zoals beschreven in \autoref{fig:chapt4_architectuur_begin}: eerst worden de geschreven reviews door een transformermodel omgezet naar numerieke features. Deze worden dan toegevoegd aan de input van een neuraal netwerk. We maken dus gebruik van een 'feature augmentation'  hybride model (\ref{sec:chapt2_hybride_modellen}) met machine learning-technieken (\ref{sec:chapt2_machine_learning_modellen}). De overige features voor het neurale netwerk komen uit de dataset, eventueel verwerkt met feature engineering. Dit betreft dan features die de restaurants beschrijven, zoals het type restaurant (bvb "fastfood"). In \autoref{sec:chapt4_tekst_naar_features} beschrijven we welke combinatie van technieken het beste werkt om de geschreven reviews om te zetten naar features voor het neurale netwerk. In \autoref{sec:chapt4_neuraal_netwerk} doen we onderzoek naar de optimale vorm van het neurale netwerk om de scores zo precies mogelijk te kunnen voorspellen.

% todo figuur aanpassen: BERT -> BERTopic?
\mijnfiguur[H]{width=12cm}{fig/chapt4/predictor/architectuur_begin.png}{Schets van de initiële architectuur}{fig:chapt4_architectuur_begin}
% TODO: throwback naar chapt2 waarin ik de uitdagingen uitleg en geef aan in welke categorie we zitten

\section{Data}
\label{sec:chapt4_data_test_train_uitleg}
% TODO: uitleggen hoe onze train-test shit werkt, als dat werkt. Best met een afbeelding om het wat duidelijk te maken.

\section{Tekst naar features}
\label{sec:chapt4_tekst_naar_features}
In deze sectie zullen we beschrijven hoe we uit de tekstuele reviews features zullen verkrijgen. Ook beschrijven we de redeneringen achter de algoritmen, met andere woorden wat deze features moeten voorstellen. We zullen deze algoritmen vooral baseren op BERTopic beschreven in \autoref{sub:chapt2_bertopic}. Aangezien we verschillende modellen zullen verkrijgen moeten we deze zo objectief mogelijk evalueren, hoe we dit doen staat beschreven in \autoref{sub:chapt4_testsetup}. BERTopic zal van de reviews een clustering maken, dit volstaat nog niet als features voor het neurale netwerk. We zullen van deze clustering eerst nog gebruikers- en restaurantprofielen moeten maken. Dit proces is ook gevisualiseerd in \autoref{fig:chapt4_structuur_evaluatie_bertopic}.

\mijnfiguur[H]{width=16cm}{fig/chapt4/NLP/structuur_evaluatie_bertopic.jpg}{Visualisatie van het proces om tekstuele reviews om te zetten in de uiteindelijk gebruikers- en restaurantprofielen.}{fig:chapt4_structuur_evaluatie_bertopic}

% TODO: Arnoud, go ahead. Bespreek in deze section ook al de individuele resultaten. Random grafieken: go! Chapter 5 is enkel bedoelt om alles nog eens samen te vatten.
\subsection{Testset-up}
\label{sub:chapt4_testsetup}
Zoals gevisualiseerd in \autoref{fig:chapt4_structuur_evaluatie_bertopic} kunnen we ons algoritme op meerdere plaatsen evalueren. In volgende paragrafen zullen we beide manieren met elkaar vergelijken. We zullen onder andere de werking beschrijven, enkele voor- en nadelen oplijsten en een conclusie trekken.

\subsubsection{Evaluatie van profielen}

Een eerste mogelijkheid is om de uiteindelijke verkregen features te evalueren. In dit geval zullen dat de gebruikers- en restaurantprofielen zijn, zoals beschreven in \autoref{sub:chapt2_gebruikersprofielen}. Aan de ene kant modelleren ze wat een bepaalde gebruiker belangrijk vindt via zijn gebruikersprofiel. Aan de andere kant zullen we dit combineren met een restaurantprofiel, hiermee geven we de specialiteiten en andere eigenschappen van een bepaald restaurant weer. Om deze profielen te evalueren zullen we gebruik maken van de architectuur afgebeeld in \autoref{fig:chapt4_architectuur_begin}. We zullen het volledige neurale netwerk constant houden met uitzondering van deze profielen. Op deze manier kunnen we de profielen objectief beoordelen door de output van het neurale netwerk te evalueren. Ten slotte zullen we de (combinatie van) profielen waarvoor het model het beste presteert selecteren.

\subsubsection{Evaluatie van de clustering}

De tweede manier is aan de hand de verkregen clusters zoals beschreven in \autoref{sub:chapt2_bertopic_clustering}. Bij de keuze van een evaluatiemetriek voor deze clusters moeten we rekening houden met enkele aspecten. Het eerste is dat we geen gelabelde data hebben. In theorie kunnen we de zinnen handmatig labelen, helaas zal dit niet altijd even accuraat zijn. Hierbij komt ook nog het probleem dat elk BERTopic model verschillende topics zal maken, het gevolg is dat we handmatig gelabelde data niet kunnen hergebruiken. Met deze redenen zullen we metrieken die gebruik maken van de ground truth labels uitsluiten en gebruik maken van de technieken beschreven in \autoref{sec:chapt2_clustering_evaluation}.

% TODO REF: https://towardsdatascience.com/7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2

% TODO subsecties?
% TODO voordelen/nadelen van technieken
% sneller te weten of het goed is of niet
% trager (trainen neuraal netwerk)
% geen expliciet gebruik van profielen bij clustering
% TODO schalen de clusteringsmetrieken? -> nee, maar is het nodig om dit op zoveel data te doen?

% TODO ZIJN DE VOLGORDE VAN GOED NAAR SLECHT GELIJKAARDIG?

% TODO uitleggen nadelen van bertopic overwinnen -> per zin splitsen => meerdere topics
% TODO: technische details: framework, libraries, python versie, cpu model, OS, ... + eventueel korte motivatie voor keuze (gelijk 1-2 zinnen, zoals HuggingFace is in de NLP community de standaard om modellen uit te wisselen, ik zeg maar iets)

% TODO: Arnoud moet vermelden in HS4 dat wij geen finetuning doen omdat er geen specifieke dataset bestaat voor ons

\subsection{Clustering via BERTopic}
In deze sectie zullen we bespreken hoe we BERTopic zullen gebruiken om een clustering te genereren. Deze modellen zullen we dan gebruiken bij het creëren van gebruikers- en restaurantprofielen in \autoref{sec:chapt4_nlp_profielen}. We hebben gebruik gemaakt van een bestaande implementaties van BERTopic \cite{bertopic_homepage} waarbij we de verschillende lagen aanpassen door bijvoorbeeld sentence-BERT te gebruiken van \cite{sentence_transformers_implementation}. 

% todo in elke titel van subsubsectie BERTOPIC?
\subsubsection{Standaard BERTopic}
Het initiële model zal gebruik maken van de standaardimplementatie, zoals beschreven in \autoref{sub:chapt2_bertopic}, in combinatie met sentence-BERT. Deze implementatie is voor de volledigheid gevisualiseerd in \autoref{fig:basismodel_bertopic}. Bovenop dit model zullen we ook nog een extra finetuning laag toevoegen, namelijk KeyBERTInsipired beschreven in \autoref{sub:topic_representatie}

\mijnfiguur[H]{width=5cm}{fig/chapt4/NLP/basismodel_bertopic.jpg}{Visualisatie van de standaardimplementatie van BERTopic \cite{bertopic_algo}.}{fig:basismodel_bertopic}

De eerste stap is het bepalen wat de documenten zullen voorstellen, hiervoor hebben we meerdere mogelijkheden. De meest voor de hand liggende mogelijkheid is dat we één review als een document beschouwen. Dit zal betekenen dat we ongeveer 4,7 miljoen documenten hebben zoals beschreven in \autoref{sub:chapt3_eigenschappen_dataset}. Helaas brengt deze aanpak meerdere problemen met zicht mee. 

Één probleem volgt rechtstreeks uit het gebruik van BERTopic, zoals beschreven in \autoref{sub:chapt2_bertopic} kunnen we namelijk een document maar aan één cluster toevoegen. Dit is tegenstrijdig met de praktijk, waar reviews meerdere onderwerpen aankaarten zoals lekker eten maar slechte service. Aangezien een cluster overeenkomt met precies één onderwerp is dit geen ideale match. We zullen deze complicatie deels vermijden door de reviews op te splitsen in zinnen. Dit zal gebeuren via een tokenizer zoals beschreven in \autoref{sub:chapt2_tokenization}. Hierbij veronderstellen we wel nog steeds dat één zin overeenkomt met één onderwerp. Gebaseerd op een steekproef lijkt dit voor de meeste zinnen uit de reviews wel het geval. Onze implementatie gebruikt de sentence tokenizer van SpaCy \cite{spacy_main}.

Via deze methode kunnen we aan elk van de 36 miljoen zinnen een cluster toekennen, hierdoor kunnen we meerdere onderwerpen aan één review toekennen. Voor de rest van deze masterthesis zullen we één document gelijkstellen aan één zin uit een review. Een bijkomend nadeel van deze methode is dat sommige zinnen uit geen enkel relevant onderwerp bestaan. Dit zijn zinnen die, ongerelateerd aan het restaurant, een verhaal vertellen of bepaalde omstandigheden omschrijven. Het gevolg kan waargenomen worden in de representatie van enkele topics, deze brengen geen waardevolle informatie. 

De volgende uitdaging is de schaalbaarheid van het algoritme. Zoals beschreven in vorige paragraaf hebben we ongeveer 36 miljoen documenten die we zullen moeten clusteren. Voor het genereren van de embeddings zal dit al snel een probleem geven wegens de grote hoeveelheid verreist geheugen. Deze hoeveelheid zal nog toenemen eens we een clustering zullen creëren. Voor ongeveer 2\% van de data (100 000 reviews gelijk aan 650 000-700 000 documenten) is dit nog mogelijk met een geheugen van 64GB. Vervolgens kunnen we de overige documenten bevragen aan de hand van het getrainde model.

% todo prestatie grafiek: OFFLINE_BERTOPIC
% todo paragraaf resultaat?


% TODO implementation optimizations => caching scores?

% todo we use Azure cluster for offline BERT, yes yes
% todo potentielepotentielepotentielepotentielepotentielepotentiele verklaring -> embeddings zijn niet gefinetuned op eten => overlap CLUSTERINGSMETRICS

% TODO:
% Sentiment analysis H2 en verschil in user/restaurant profiles (grafiek go brrr)
% Bertopic to userprofile
% Countvectorizer + stopwords en lemmatization voor approximations

% ref sentence_transformers_implementation
% ref bertopic_homepage
% ref sentiment: Huggingface

% TODO zero-shot encoding


\subsubsection{Online BERTopic}
Een online variant, ook wel incrementeel genoemd, is een algoritme dat gebruik kan maken van een datastroom zonder de volledige input te weten. Hierdoor kan het algoritme de data in kleinere delen verwerken, wat in ons geval interessant is. Een bijkomend voordeel hiervan is de mogelijkheid om toekomstige data efficiënt te verwerken. We zullen door het model kunnen updaten met nieuwe data zonder alles opnieuw te moeten trainen. In ons geval is dit niet relevant, maar dit is zeker een nuttige eigenschap voor een productieomgeving.

Om BERTopic om te zetten naar een online algoritme zullen we alle stappen moeten omzetten naar een online variant, indien dit nog niet het geval is. Hoe deze stappen omgezet worden staan hieronder beschreven, deze transformatie wordt ook gevisualiseerd in \autoref{fig:chapt4_bertopic_online_transformation}

% todo REF naar H2 indien we het daar uitleggen.
\begin{itemize}
    \item \textbf{De embedding} komt van een LLM, deze hoeft niet continue getrained te worden en kan al bevraagd worden voor ongeziene data. Hiervoor is er dus geen aanpassing nodig
    \item Voor \textbf{dimensionaliteitsreductie} gebruiken we een online variant, namelijk IPCA.
    \item Bij het \textbf{clusteringsalgoritme} schakelen we over naar een online K-Means algoritme genaamd MiniBatch K-Means.
    \item \textbf{De BOW representatie} wordt ook aangepast naar zijn online variant.
    \item Aangezien \textbf{c-TF-IDF} en verdere aanpassingen gebaseerd zijn op de BOW representatie zal deze online zijn indien het BOW algoritme online werkt.
\end{itemize}

\mijnfiguur[H]{width=12cm}{fig/chapt4/NLP/bertopic_to_online.jpg}{Transformatie van de standaard BERTopic structuur naar een online algoritme.}{fig:chapt4_bertopic_online_transformation}

Door het gebruik van deze structuur kunnen we het model op een grotere hoeveelheid data trainen. Dit model kunnen we trainen op de volledige dataset, aangezien we hier  geen rechtstreekse features genereren voor het neurale netwerk.

% wat zijn de resultaten
% voor/nadelen -> trainingstijd, performance, schaalbaarheid
%

\subsubsection{Guided BERTopic}
Een laatste experiment voor clustering maakt gebruik van guided BERTopic \cite{bertopic_guided}, een reeds geïmplementeerde variant op het standaard algoritme. Hierbij geven we per topic een lijst van woorden mee die het onderwerp van deze topic voorstellen. Deze lijst van woorden wordt de seed van de topic genoemd. Het uiteindelijke model houdt hiermee rekening en zal de kans vergroten om deze seeded topics als finale output te genereren. Merk op dat dit volgens de auteur niet altijd het geval is. Vaak zullen deze topics aangepast of opgesplitst worden, tenzij de ze extreem accuraat zijn.

Via deze methode willen we het probleem van irrelevante topics, als gevolg van het opsplitsen in zinnen, vermijden. We doen dit door naast de vaste lijst van topics, het model ruimte te geven om extra topics te genereren. Het doel is dat deze topics gevuld worden met de irrelevante zinnen.

Na enkele pogingen met verschillende parameters zien we telkens hetzelfde probleem opduiken. Door de grote hoeveelheid data zien we dat de voorgedefinieerde topics overspoeld worden met andere data. Hierdoor blijft er weinig over van de originele seeds. Een mogelijke oplossing is om minder data te gebruiken, hierdoor kunnen we ook gebruik maken van de standaardimplementatie van BERTopic. Door minder data te gebruiken zijn de resultaten significant slechter, dit zullen we uitgebreid bespreken in \autoref{sub:chapt4_nlp_resultaten}.


\subsection{Gebruikers- en restaurantprofielen}
\label{sec:chapt4_nlp_profielen}
De uiteindelijke features die we in het model gebruiken worden opgesteld op basis van het BERTopic model. Deze features bestaan uit een vector voor elke gebruiker en elk restaurant, we noemen ze respectievelijk gebruikers- en restaurantprofielen. Het BERTopic model zelf maakt gebruik van de volledige dataset, dit betekent niet dat de profielen dit ook doen. Deze worden met een deel van de data opgesteld, zoals beschreven in \autoref{TODO}. \newline
Het opstellen kan op twee manier gebeuren. De eerste manier spreekt voor zich, deze maakt simpelweg gebruik van het model door de reviews zelf te clusteren en op basis daarvan een profiel op te stellen. De andere manier zal geen gebruik maken van de clustering. Deze zal een profiel afleiden uit de topic representaties van het getrainde BERTopic model. Ten slotte maken we een onderscheid tussen een gebruikers en een restaurant. Dit komt omdat de stappen om het profiel op te stellen licht kunnen wijzigen door bijvoorbeeld sentiment analysis toe te passen.


% TWEE MANIEREN -> topics / approx : lengte=aantal topics
% ===>>> MAAK MOOIE IMAGES DIE DE STAPPEN MOOI AANTONEN

% TODO PARAMETER SELECTION -> SIZE OF MODEL: 
% MANUAL SELECTION IN APPROX

\subsubsection{Profiel op basis van de clustering}
% VIA CLUSTERING
% clusters ci bepalen voor elke zin
% tellen hoeveel keer elke cluster gekozen werd per user:
% TOEVOEGING sentiment: positief/negative => +1 en -1 of gewogen via de confidence score -> performance met/zonder (MOET via neuraal netwerk)
% groupby reviews => KEUZE normalizeren VS later doen -> elke review evenveel impact vs elke zin evenveel impact
% indien genormalizeerd => gemiddelde nemen van alle reviews van de user => elke review heeft evenveel impact
% indien niet genormalizeerd => alles sommeren en dan normalizeren => gevolg is dat elke zin evenveel impact heeft.

\subsubsection{Profiel op basis van de representaties}
% VIA APPROXIMATION
% uit de representaties van de cluster VS document (MERK OP DAT DIT OOK KAN PER REVIEW, wij hebben het per zin gedaan (sommige zinnen meerdere onderwerpen)
% estimaties voor elke topic = SOM is 1
% neem de beste N per topic (door de anderen op 0 te zetten)
% OPTIONELE STAP -> normalizatie zodat de som van deze topics 1 is
% analoog aggregeren per review en dan per user/restaurant
% ten slotte het elk profiel normalizeren (tussen 0-1)

% voordeel => NOG meer topics per zin!

\subsubsection{Gebruikers tegenover restaurants}
% verschillen user/restaurant
% sentiment
% mogelijks approx VS topic => geen idee qua verklaring
% manueel filteren => verschil!


\subsection{Resultaten}
\label{sub:chapt4_nlp_resultaten}
% todo praat over brol topics -> in principe niet erg => neuraal netwerk leert dat er uit filteren
% todo vergelijk resultaten hier



\section{Neuraal netwerk}
\label{sec:chapt4_neuraal_netwerk}

% TODO: waarom een neuraal netwerk? Welke voordelen spreken ons aan?
% TODO: Arno, go ahead. Bespreek in deze section ook al de individuele resultaten. Random grafieken: go! Chapter 5 is enkel bedoelt om alles nog eens samen te vatten
\subsection{Input}
Zoals aangeduid in \autoref{fig:chapt4_architectuur_begin} zijn er twee bronnen van inputdata voor het neurale netwerk: de labels rechtstreeks geëxtraheerd uit de Yelp dataset, en de geschreven reviews die zijn omgezet naar numerieke features zoals beschreven in \autoref{sec:chapt4_nlp_profielen}. Beide bronnen modelleren steeds zowel een gebruiker als een restaurant. Met deze data moet het neuraal netwerk een voorspelling maken welke score die specifieke gebruiker aan dat specifieke restaurant geeft.

Een neuraal netwerk aanvaardt enkel numerieke features. Bij de NLP gebruikers- en restaurantprofielen is dit reeds opgelost. Bij de labels is er meer werk. We bespreken eerst hoe een restaurant gemodelleerd wordt, en daarna hoe we deze modelleren kunnen aanpassen om ook gebruikersdata te ondersteunen.
% TODO: checken dat er niet te veel overlap is met Arnoud hier
\subsubsection{Restaurantlabels}
\label{sec:chapt4_nn_restaurantlabels}
Een restaurant wordt hoofdzakelijk beschreven in de Yelp dataset met behulp van categorieën en attributen (\autoref{sec:chapt3}). We maken gebruik van one-hot encoding om de aanwezigheid van een categorie (nominaal) bij een restaurant aan te duiden. Doordat er in de totale dataset 1311 unieke categorieën zijn zou er door de one-hot encoding een zeer ijle inputvector gemaakt worden. Om dit probleem te beperken, houden we enkel de categorieën die bij minstens 500 restaurants (1\%) voorkomen. Zo houden we nog 75 categorieën over.

Niet ieder restaurant beschikt over een waarde voor ieder attribuut. Ongeveer 33\% ontbreekt minstens één attribuut. Ordinale data, zoals de prijsklasse, wordt omgezet naar een waarde in $[0, 1]$ die overeenkomt met de rangorde. Voor de andere attributen gebruiken we opnieuw one-hot encoding. In tegenstelling tot categorieën moeten we rekening houden dat de afwezigheid van een attribuut in de dataset niet per se overeenkomt met het werkelijk ontbreken van dat attribuut in de echte wereld. Aangezien deze data wel waardevol lijkt voor aanbevelingen, lossen we dit probleem op door een standaardwaarde per attribuut in te stellen. Zo wordt de afwezigheid van een one-hot encoded attribuut gemodelleerd als $0,5$.

We berekenen de huidige gemiddelde score van een restaurant en voegen deze toe als feature. De Yelp dataset bevat ook de check-ins voor ieder restaurant. Deze datapunten worden omgezet tot een numerieke waarde door het gemiddeld aantal check-ins per week te berekenen. Samen modelleren deze twee nieuwe features een rudimentaire vorm van de populariteit van een restaurant.

Het gemiddeld aantal check-ins per week bij een restaurant varieert sterk, van quasi 0 bij niche restaurants tot meer dan 100 bij grote ketens. Deze afgeleide feature ligt dus niet in $[0, 1]$ zoals alle andere features en zou ervoor zorgen dat deze feature meer doorweegt in het neurale netwerk. De spreiding van waarden van deze feature is ook niet uniform: er zijn enkele uitschieters die niet in lijn liggen met de overige restaurants. Een gewone normalisatie zal dit dus niet oplossen. We herschalen daarom alle data die tussen het 5$^e$ en 95$^e$ percentiel tot $[0, 1]$ en extreme lage en hoge waarden transformeren we tot respectievelijk $0$ en $1$. Hierdoor krijgen we een betere spreiding en heeft deze feature een even groot gewicht als alle andere.

\subsubsection{Gebruikerslabels}
Uit \autoref{sec:chapt4_nn_restaurantlabels} volgt dat we een restaurant $i$ kunnen voorstellen aan de hand van labels in de vorm van een vector $RP_i = (feature_1, feature_2, ..., feature_n)$. We bepalen nu per gebruiker de verzameling van restaurants $\mathcal{V}$ waarvoor die gebruiker een review heeft achtergelaten. We kunnen dan voor gebruiker $j$ een gebruikersprofiel $UP_j$ opstellen:
\begin{equation}
    UP_j = \frac{\sum_{v \in \mathcal{V}} (RP_v \cdot R_{j, v})}{\vert \mathcal{V} \vert}
\end{equation}

met $R_{j, v}$ de genormaliseerde score die gebruiker $j$ geeft aan restaurant $v$. De Yelp dataset bevat nog enkele andere gegevens over de gebruikers: zo wordt er bijgehouden hoeveel keer andere gebruikers een review 'nuttig', 'grappig' of 'cool' vonden. Deze metadata over een gebruiker helpt om de betrouwbaarheid van de reviews te modelleren. We creëren een nieuwe feature door de som van het aantal positieve interacties te nemen. Er zijn enkele bekende gebruikers op Yelp, die veel volgers hebben en hierdoor veel meer feedback hebben gekregen op hun reviews. Daarom herschalen we eerst de feature door alle waarden hoger dan het 99$^e$ percentiel te mappen op $1$ en de rest te normaliseren tussen $[0, 1]$.

We experimenteerden ook om de gemiddelde score van een gebruiker uit de trainset toe te voegen aan de inputvector. Echter merkten we op dat deze feature zeer dominant is tijdens training, en het model zich hier volledig naar aanpast. Om dit te voorkomen, kozen we ervoor om deze feature niet in de inputvector te steken.


Op dit punt hebben we dus alle benodigde input om aan het neurale netwerk te geven (\autoref{fig:chapt4_architectuur_input}):
\begin{itemize}
    \item gebruikersprofielen gecreëerd door NLP (geschreven reviews)
    \item restaurantprofielen gecreëerd door NLP (geschreven reviews)
    \item gebruikersprofielen gebaseerd op labels (Yelp dataset labels)
    \item restaurantprofielen gebaseerd op labels (Yelp dataset labels)
\end{itemize}

We kunnen deze data nu aan het neuraal netwerk geven om een voorspelling te genereren voor de score die een gespecificeerde gebruiker aan een gespecificeerde restaurant geeft.
\mijnfiguur[H]{width=8cm}{fig/chapt4/predictor/architectuur_input.png}{Close-up van inputlaag architectuur zoals in \autoref{fig:chapt4_architectuur_begin}}{fig:chapt4_architectuur_input}


% TODO: zie correlatiegraphs, om te bekijken of er obvious dubbele data inzit of niet


\subsection{Testset-up}
% TODO: hoe evalueer je wat een goed model is? Hoe doen we dit objectief? Welke problemen had je met het opstellen van een objectief framework?
De modellen zijn steeds geschreven in Python 3.10. De implementaties maken gebruik van PyTorch 2.0.0. \cite{pytorch} De inputvector voor het neurale netwerk bestaat uit \verb||torch.float32 getallen. De uitvoering gebeurt op een AMD Ryzen 5800X, NVIDIA RTX 2080, en 64GB werkgeheugen. % TODO: checken dat bij arnoud dit klopt, en eventueel checken dat arnoud ook de azure pc erop zet, want dat lijkt beter

We analyseren iedere implementatie op dezelfde manier: eerst wordt de data opgesplitst in een train- en testset (80\% - 20\%). % kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa kappa
Iedere epoch trainen we het neuraal netwerk met deze trainset en berekenen we vervolgens de loss op de testset. Als de loss meerdere epochs op rij omhoog gaat, stoppen we het trainen om overfitting te voorkomen. De gebruikte lossfunctie voor optimalisatie is de MSE.

Ten slotte nemen we een subset van de testset om aan de hand van het model een score tussen 1 en 5 te voorspellen. We ronden hierbij de voorspelling af. We maken een histogram van het verschil tussen de voorspelling en echte score toegekend door de gebruiker. Dit histogram gebruiken we als controle dat een lagere MSE overeenkomt met meer accurate voorspellingen.

\subsection{Basisimplementatie}
We beginnen met een basisimplementatie van een neuraal netwerk, om te verifiëren dat het mogelijk is om de echte score te schatten op basis van de inputdata.
% TODO: eerste idee en resultaten, problemen en opmerkingen...
% TODO: waarom float als output? En geen classificatie





\subsection{Uitbreidingen architectuur}
% TODO: MLP, simpel, moeilijker, custom loss functie. Bij makkelijke architectuur gaan we naar default predicition van 4 sterren, onafhankelijk van optimizer en loss functie (penalty/weights). Bij moeilijkere architectuur hebben we moeite om loss naar beneden te krijgen.

\subsection{Optimalisaties}
% TODO: optimizer, custom loss function
% TODO: learning rate optimalisatie

% TODO: maak een note dat de input het belangrijkste is voor de performance van het netwerk, en niet de architectuur zelf enzo

\subsection{Cold-startprobleem}

\subsection{Grootte dataset}

\subsection{Random Forest}
% TODO: random forest werkt ok, als we explainability belangrijk zouden vinden. RMSE van 1.2

